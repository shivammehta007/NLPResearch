{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq_Translation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNJ8mHqFx5Dfmm+7abrP8VQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivammehta007/NLPResearch/blob/master/Seq2Seq_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxZvNWTw2ywG",
        "colab_type": "text"
      },
      "source": [
        "# Sequence to Sequence Machine Translation [TUT]\n",
        "\n",
        "We will be writing an encoder-decoder model to try to Machine Translate with help of NLP and Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3rlENdh4h1C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "76e7fdd3-96d2-4d8d-be5d-67a9dbaec11b"
      },
      "source": [
        "!pip install -U tqdm\n",
        "!python -m spacy download de"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tqdm in /usr/local/lib/python3.6/dist-packages (4.43.0)\n",
            "Collecting de_core_news_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 1.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.1.0-cp36-none-any.whl size=11073065 sha256=896b80e610b854e1a41f80bcce6e91d8624485665f6d99948b887776bf2b22ff\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-eqqkqjj8/wheels/b4/8b/5e/d2ce5d2756ca95de22f50f68299708009a4aafda2aea79c4e4\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGtz_n632v-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import torchtext.data as data\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "\n",
        "import os\n",
        "import spacy\n",
        "import math\n",
        "import random\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efC09s843X6J",
        "colab_type": "text"
      },
      "source": [
        "## Seeding\n",
        "For duplication of results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE4adJFD3bK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_all(seed=1234):\n",
        "    \"\"\"Seed the results for duplication\"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "SEED = 1234\n",
        "seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFJcuztt_xqd",
        "colab_type": "text"
      },
      "source": [
        "## PreProcessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz17Wmaa_09_",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn7LT-ZU5Djq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_en = spacy.load('en')\n",
        "spacy_de = spacy.load('de')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA8uz4Tm8lSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_en(sentence):\n",
        "    return [word.text for word in spacy_en.tokenizer(sentence)]\n",
        "\n",
        "def tokenize_de(sentence):\n",
        "    return [word.text for word in spacy_de.tokenizer(sentence)][::-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HHWMalBLVpy",
        "colab_type": "text"
      },
      "source": [
        "### Data Loaders\n",
        "Create two Field Texts for Source and Desitnation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLN-NyLtLNk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source = data.Field(tokenize=tokenize_de,\n",
        "               init_token='<sos>',\n",
        "               eos_token='<eos>',\n",
        "               lower=True)\n",
        "\n",
        "destination = data.Field(tokenize=tokenize_en,\n",
        "                    init_token='<sos>',\n",
        "                    eos_token='<eos>',\n",
        "                    lower=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIdA-GtiMSyM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "bab44434-1201-46f2-8847-fd9872364b66"
      },
      "source": [
        "%%time\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(source, destination))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 667kB/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 221kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 218kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 17.8 s, sys: 169 ms, total: 18 s\n",
            "Wall time: 22.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrtYI2-YM_Dy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0ab2882-9a3a-4dd6-899e-62df41b566d4"
      },
      "source": [
        "len(train_data), len(valid_data), len(test_data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29000, 1014, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYTRxtIzPgIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source.build_vocab(train_data, min_freq=2)\n",
        "destination.build_vocab(train_data, min_freq=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ2jPLhORFy7",
        "colab_type": "text"
      },
      "source": [
        "#### Set up Device, CPU or GPU\n",
        "To put Iterator onto that device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5_jFFj3Pr-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a9dc9a5-0cc8-46a7-d4a5-93788654584b"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiYOVCVUPs-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(datasets=(train_data,valid_data, test_data),\n",
        "                                                                           batch_size=BATCH_SIZE, \n",
        "                                                                           device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu26JzGKR6Cz",
        "colab_type": "text"
      },
      "source": [
        "## Building the Model\n",
        "\n",
        "For starters this is uni-directional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq3tQtpbR8TL",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it-04XjbRPcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder RNN for the Seq2Seq Model\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, embedding_dim, hid_dim, n_layers, dropout):\n",
        "        super(self, Encoder).__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.hid_dim = hid_dim\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, source_sentence):\n",
        "        embedding  = self.embedding(source_sentence)\n",
        "        outputs, (hidden, cell) = self.lstm(embedding)\n",
        "        return hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaVbje3wWQ37",
        "colab_type": "text"
      },
      "source": [
        "#### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8jT7lHnWP_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder RNN for the Seq2Seq Model\"\"\"\n",
        "    \n",
        "    def __init__(self, output_dim, embedding_dim, hid_dim, n_layers, dropout):\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.embedding(input)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ib4yk5JiOYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}