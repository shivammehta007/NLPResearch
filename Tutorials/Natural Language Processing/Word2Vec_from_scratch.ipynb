{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DUd48lMmcPv",
        "colab_type": "text"
      },
      "source": [
        "# Writing Word2Vec Implementation from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40h6kcMEmNoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vNVxez_xTgQ",
        "colab_type": "text"
      },
      "source": [
        "## Preparing Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjqE3Skjmjfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = 'natural language processing and word to vector learning with the help of this text'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM9amVrpxIAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [[word.lower() for word in text.split()]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj4d87fWxWsx",
        "colab_type": "text"
      },
      "source": [
        "## HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCokSCiwxNd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "settings = {\n",
        "\t'window_size': 2,\n",
        "\t'n': 10,\t\t# Dimensions of word embeeding / size of hidden layer\n",
        "\t'epochs': 50,\t\t\n",
        "\t'learning_rate': 0.01\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU_mz0n6YdYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2l9WWwkxcxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2Vec:\n",
        "    def __init__(self, window_size=2, embedding_size=10, epochs=50, learning_rate=0.01):\n",
        "        self.window_size = window_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.word_count = defaultdict(int)\n",
        "        self.word_size = 0\n",
        "        self.w1 = np.array([])\n",
        "        self.w2 = np.array([])\n",
        "        self.word2index = {}\n",
        "        self.index2word = {}\n",
        "\n",
        "    \n",
        "    def populate_word_count(self, corpus):\n",
        "        for line in corpus:\n",
        "            for word in line.split():\n",
        "                self.word_count[word] += 1\n",
        "\n",
        "        self.word_size += 1\n",
        "        logging.debug('Word Count Generated')\n",
        "        return converted2onehot(corpus)\n",
        "\n",
        "    def converted2onehot(self, corpus):\n",
        "        if not self.word_count:\n",
        "            raise NotImplementedError('Word Count has not been populated, \\\n",
        "            use populate_word_count() method first')\n",
        "\n",
        "        for i in range(len(corpus))\n",
        "            corpus[i] = [self.word_count[word] for word in corpus[i].split()]\n",
        "\n",
        "        return corpus\n",
        "\n",
        "    def forward_propogate(self, x):\n",
        "        h = np.dot(self.w1.T, x)\n",
        "        u = np.dot(self.w2.T, h)\n",
        "        y_c = self.softmax(u)\n",
        "        return y_c, h, u\n",
        "\n",
        "    def train(self, corpus, log_after_n_epochs=5):\n",
        "        populate_word_count(corpus)\n",
        "        logging.debug('Initializing wiehgts based on the length of corpus')\n",
        "        self.w1 = np.random.uniform(-0.8, 0.8, (self.word_size, self.embedding_size))\n",
        "        self.w1 = np.random.uniform(-0.8, 0.8, (self.embedding_size, self.word_size))\n",
        "        \n",
        "        for i in range(self.epochs):\n",
        "            self.loss = 0\n",
        "            for i, text in enumerate(self.)\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}