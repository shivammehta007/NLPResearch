{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = 'Hello Shivam Mehta! How are you? How is it going? everything is alright lets learn something new? I am fine thank you. It was pleasure to meet you. I am from India. Currently studying in ITMO University in Saint Petersburg, Russia.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: ['Hello Shivam Mehta!', 'How are you?', 'How is it going?', 'everything is alright lets learn something new?', 'I am fine thank you.', 'It was pleasure to meet you.', 'I am from India.', 'Currently studying in ITMO University in Saint Petersburg, Russia.']\n",
      "words : ['Hello', 'Shivam', 'Mehta', '!', 'How', 'are', 'you', '?', 'How', 'is', 'it', 'going', '?', 'everything', 'is', 'alright', 'lets', 'learn', 'something', 'new', '?', 'I', 'am', 'fine', 'thank', 'you', '.', 'It', 'was', 'pleasure', 'to', 'meet', 'you', '.', 'I', 'am', 'from', 'India', '.', 'Currently', 'studying', 'in', 'ITMO', 'University', 'in', 'Saint', 'Petersburg', ',', 'Russia', '.']\n"
     ]
    }
   ],
   "source": [
    "print('sentence: {}'.format(sent_tokenize(example_sentence)))\n",
    "print('words : {}'.format(word_tokenize(example_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'shivam', 'mehta', '!', '?', 'going', '?', 'everything', 'alright', 'lets', 'learn', 'something', 'new', '?', 'fine', 'thank', '.', 'pleasure', 'meet', '.', 'india', '.', 'currently', 'studying', 'itmo', 'university', 'saint', 'petersburg', ',', 'russia', '.']\n"
     ]
    }
   ],
   "source": [
    "english_stop_words = set(stopwords.words('english'))\n",
    "filtered_words = list(filter(lambda x: x not in english_stop_words, word_tokenize(example_sentence.lower())))\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'python', 'python', 'python', 'pythonli']\n"
     ]
    }
   ],
   "source": [
    "stemming_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\" ]\n",
    "print(list(map(ps.stem, stemming_words )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming the example text we considering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'shivam', 'mehta', '!', '?', 'go', '?', 'everyth', 'alright', 'let', 'learn', 'someth', 'new', '?', 'fine', 'thank', '.', 'pleasur', 'meet', '.', 'india', '.', 'current', 'studi', 'itmo', 'univers', 'saint', 'petersburg', ',', 'russia', '.']\n"
     ]
    }
   ],
   "source": [
    "print(list(map(ps.stem,filtered_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging ( POS Tagging)\n",
    "\n",
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction  \n",
    "CD\tcardinal digit  \n",
    "DT\tdeterminer  \n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")  \n",
    "FW\tforeign word  \n",
    "IN\tpreposition/subordinating conjunction  \n",
    "JJ\tadjective\t'big'  \n",
    "JJR\tadjective, comparative\t'bigger'   \n",
    "JJS\tadjective, superlative\t'biggest'  \n",
    "LS\tlist marker\t1)  \n",
    "MD\tmodal\tcould, will  \n",
    "NN\tnoun, singular 'desk'  \n",
    "NNS\tnoun plural\t'desks'  \n",
    "NNP\tproper noun, singular\t'Harrison'  \n",
    "NNPS\tproper noun, plural\t'Americans'  \n",
    "PDT\tpredeterminer\t'all the kids'  \n",
    "POS\tpossessive ending\tparent\\'s  \n",
    "PRP\tpersonal pronoun\tI, he, she  \n",
    "PRP\\\\$\tpossessive pronoun\tmy, his, hers  \n",
    "RB\tadverb\tvery, silently,  \n",
    "RBR\tadverb, comparative\tbetter  \n",
    "RBS\tadverb, superlative\tbest  \n",
    "RP\tparticle\tgive up  \n",
    "TO\tto\tgo 'to' the store.  \n",
    "UH\tinterjection\terrrrrrrrm  \n",
    "VB\tverb, base form\ttake  \n",
    "VBD\tverb, past tense\ttook  \n",
    "VBG\tverb, gerund/present participle\ttaking  \n",
    "VBN\tverb, past participle\ttaken  \n",
    "VBP\tverb, sing. present, non-3d\ttake  \n",
    "VBZ\tverb, 3rd person sing. present\ttakes  \n",
    "WDT\twh-determiner\twhich  \n",
    "WP\twh-pronoun\twho, what  \n",
    "WP\\\\$\tpossessive wh-pronoun\twhose  \n",
    "WRB\twh-abverb\twhere, when  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import state_union\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sentence_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Shivam Mehta!', 'How are you?', 'How is it going?', 'everything is alright lets learn something new?', 'I am fine thank you.', 'It was pleasure to meet you.', 'I am from India.', 'Currently studying in ITMO University in Saint Petersburg, Russia.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = sentence_tokenizer.tokenize(example_sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged = list(map(nltk.pos_tag, [word_tokenize(sentence) for sentence in tokenized_sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Hello', 'NNP'), ('Shivam', 'NNP'), ('Mehta', 'NNP'), ('!', '.')], [('How', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('?', '.')], [('How', 'WRB'), ('is', 'VBZ'), ('it', 'PRP'), ('going', 'VBG'), ('?', '.')], [('everything', 'NN'), ('is', 'VBZ'), ('alright', 'JJ'), ('lets', 'NNS'), ('learn', 'VBP'), ('something', 'NN'), ('new', 'JJ'), ('?', '.')], [('I', 'PRP'), ('am', 'VBP'), ('fine', 'JJ'), ('thank', 'NN'), ('you', 'PRP'), ('.', '.')], [('It', 'PRP'), ('was', 'VBD'), ('pleasure', 'NN'), ('to', 'TO'), ('meet', 'VB'), ('you', 'PRP'), ('.', '.')], [('I', 'PRP'), ('am', 'VBP'), ('from', 'IN'), ('India', 'NNP'), ('.', '.')], [('Currently', 'RB'), ('studying', 'VBG'), ('in', 'IN'), ('ITMO', 'NNP'), ('University', 'NNP'), ('in', 'IN'), ('Saint', 'NNP'), ('Petersburg', 'NNP'), (',', ','), ('Russia', 'NNP'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Matching Regular Expression for chunking\n",
    "chunk_gram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "chunk_parser = nltk.RegexpParser(chunk_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (Chunk Hello/NNP Shivam/NNP Mehta/NNP) !/.)\n",
      "(S How/WRB are/VBP you/PRP ?/.)\n",
      "(S How/WRB is/VBZ it/PRP going/VBG ?/.)\n",
      "(S\n",
      "  everything/NN\n",
      "  is/VBZ\n",
      "  alright/JJ\n",
      "  lets/NNS\n",
      "  learn/VBP\n",
      "  something/NN\n",
      "  new/JJ\n",
      "  ?/.)\n",
      "(S I/PRP am/VBP fine/JJ thank/NN you/PRP ./.)\n",
      "(S It/PRP was/VBD pleasure/NN to/TO meet/VB you/PRP ./.)\n",
      "(S I/PRP am/VBP from/IN (Chunk India/NNP) ./.)\n",
      "(S\n",
      "  Currently/RB\n",
      "  studying/VBG\n",
      "  in/IN\n",
      "  (Chunk ITMO/NNP University/NNP)\n",
      "  in/IN\n",
      "  (Chunk Saint/NNP Petersburg/NNP)\n",
      "  ,/,\n",
      "  (Chunk Russia/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "for item in pos_tagged:\n",
    "    chunked = chunk_parser.parse(item)\n",
    "#     chunked.draw()\n",
    "    print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (Chunk Hello/NNP Shivam/NNP Mehta/NNP !/.))\n",
      "(S (Chunk How/WRB) are/VBP (Chunk you/PRP ?/.))\n",
      "(S (Chunk How/WRB) is/VBZ (Chunk it/PRP) going/VBG (Chunk ?/.))\n",
      "(S\n",
      "  (Chunk everything/NN)\n",
      "  is/VBZ\n",
      "  (Chunk alright/JJ lets/NNS)\n",
      "  learn/VBP\n",
      "  (Chunk something/NN new/JJ ?/.))\n",
      "(S (Chunk I/PRP) am/VBP (Chunk fine/JJ thank/NN you/PRP ./.))\n",
      "(S\n",
      "  (Chunk It/PRP)\n",
      "  was/VBD\n",
      "  (Chunk pleasure/NN)\n",
      "  to/TO\n",
      "  meet/VB\n",
      "  (Chunk you/PRP ./.))\n",
      "(S (Chunk I/PRP) am/VBP from/IN (Chunk India/NNP ./.))\n",
      "(S\n",
      "  (Chunk Currently/RB)\n",
      "  studying/VBG\n",
      "  in/IN\n",
      "  (Chunk ITMO/NNP University/NNP)\n",
      "  in/IN\n",
      "  (Chunk Saint/NNP Petersburg/NNP ,/, Russia/NNP ./.))\n"
     ]
    }
   ],
   "source": [
    "chink_gram = r\"\"\"Chunk: {<.*>+}\n",
    "                        }<VB.? |IN|DT|TO>+{\"\"\"\n",
    "chink_parser = nltk.RegexpParser(chink_gram)\n",
    "for item in pos_tagged:\n",
    "    chinked = chink_parser.parse(item)\n",
    "    print(chinked)\n",
    "#     chinked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (PERSON Hello/NNP) (PERSON Shivam/NNP Mehta/NNP) !/.)\n",
      "(S How/WRB are/VBP you/PRP ?/.)\n",
      "(S How/WRB is/VBZ it/PRP going/VBG ?/.)\n",
      "(S\n",
      "  everything/NN\n",
      "  is/VBZ\n",
      "  alright/JJ\n",
      "  lets/NNS\n",
      "  learn/VBP\n",
      "  something/NN\n",
      "  new/JJ\n",
      "  ?/.)\n",
      "(S I/PRP am/VBP fine/JJ thank/NN you/PRP ./.)\n",
      "(S It/PRP was/VBD pleasure/NN to/TO meet/VB you/PRP ./.)\n",
      "(S I/PRP am/VBP from/IN (GPE India/NNP) ./.)\n",
      "(S\n",
      "  Currently/RB\n",
      "  studying/VBG\n",
      "  in/IN\n",
      "  (ORGANIZATION ITMO/NNP University/NNP)\n",
      "  in/IN\n",
      "  (GPE Saint/NNP Petersburg/NNP)\n",
      "  ,/,\n",
      "  (GPE Russia/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "for sentence in pos_tagged:\n",
    "    entity_recog = nltk.ne_chunk(sentence)\n",
    "    print(entity_recog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Shivam', 'Mehta', '!']\n",
      "['How', 'are', 'you', '?']\n",
      "['How', 'is', 'it', 'going', '?']\n",
      "['everything', 'is', 'alright', 'let', 'learn', 'something', 'new', '?']\n",
      "['I', 'am', 'fine', 'thank', 'you', '.']\n",
      "['It', 'wa', 'pleasure', 'to', 'meet', 'you', '.']\n",
      "['I', 'am', 'from', 'India', '.']\n",
      "['Currently', 'studying', 'in', 'ITMO', 'University', 'in', 'Saint', 'Petersburg', ',', 'Russia', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for sentence in pos_tagged:\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word[0]) for word in sentence]\n",
    "    print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/MasterThesisEnv/lib/python3.6/site-packages/nltk/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet Corpus\n",
    "One of the most usefull in nltk for synoms antonyms , definitions , examples etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
     ]
    }
   ],
   "source": [
    "synset = wordnet.synsets('program')\n",
    "print(synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good.n.01\n",
      "[Lemma('good.n.01.good')]\n",
      "benefit\n",
      "['for your own good', \"what's the good of worrying?\"]\n"
     ]
    }
   ],
   "source": [
    "print(synset[0].name())\n",
    "print(synset[0].lemmas())\n",
    "print(synset[0].definition())\n",
    "print(synset[0].examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding out Synonyms and Antonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('good.n.01'), Synset('good.n.02'), Synset('good.n.03'), Synset('commodity.n.01'), Synset('good.a.01'), Synset('full.s.06'), Synset('good.a.03'), Synset('estimable.s.02'), Synset('beneficial.s.01'), Synset('good.s.06'), Synset('good.s.07'), Synset('adept.s.01'), Synset('good.s.09'), Synset('dear.s.02'), Synset('dependable.s.04'), Synset('good.s.12'), Synset('good.s.13'), Synset('effective.s.04'), Synset('good.s.15'), Synset('good.s.16'), Synset('good.s.17'), Synset('good.s.18'), Synset('good.s.19'), Synset('good.s.20'), Synset('good.s.21'), Synset('well.r.01'), Synset('thoroughly.r.02')]\n"
     ]
    }
   ],
   "source": [
    "synset = wordnet.synsets('good')\n",
    "print(synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym = []\n",
    "antonym = []\n",
    "for syn in synset:\n",
    "    for l in syn.lemmas():    \n",
    "        synonym.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonym.extend([x.name() for x in l.antonyms()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'right', 'in_force', 'salutary', 'ripe', 'honorable', 'sound', 'serious', 'goodness', 'trade_good', 'full', 'soundly', 'expert', 'safe', 'in_effect', 'dear', 'dependable', 'good', 'adept', 'practiced', 'beneficial', 'respectable', 'near', 'honest', 'secure', 'upright', 'skilful', 'thoroughly', 'unspoilt', 'commodity', 'estimable', 'well', 'just', 'unspoiled', 'undecomposed', 'effective', 'proficient', 'skillful'}\n",
      "{'evil', 'badness', 'evilness', 'ill', 'bad'}\n"
     ]
    }
   ],
   "source": [
    "print(set(synonym))\n",
    "print(set(antonym))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Similarities between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7272727272727273"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = wordnet.synset('bike.n.01')\n",
    "w2 = wordnet.synset('bicycle.n.01')\n",
    "w1.wup_similarity(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = wordnet.synset('bike.n.01')\n",
    "w2 = wordnet.synset('car.n.01')\n",
    "w1.wup_similarity(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = wordnet.synset('bike.n.01')\n",
    "w2 = wordnet.synset('cat.n.01')\n",
    "w1.wup_similarity(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6956521739130435"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = wordnet.synset('bike.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "w1.wup_similarity(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
